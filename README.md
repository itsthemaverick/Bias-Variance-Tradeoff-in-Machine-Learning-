# Biasâ€“Variance Tradeoff in Machine Learning  
### Underfitting, Overfitting, Regularization & Cross-Validation (scikit-learn)

## ğŸ“Œ Overview
This repository demonstrates one of the most fundamental challenges in Machine Learning â€” the **Biasâ€“Variance Tradeoff** â€” using controlled experiments, visualizations, and production-style scikit-learn pipelines.

Instead of relying on definitions alone, this project builds models of increasing complexity, shows where and why they fail, and then applies **regularization** and **cross-validation** to systematically fix those failures.

The focus is on **generalization**, not just training accuracy.

---

## ğŸ¯ Objectives
- Demonstrate underfitting and overfitting visually
- Explain bias and variance through loss behavior
- Apply L1 and L2 regularization to control complexity
- Use K-Fold cross-validation for model selection
- Implement everything using clean, reusable scikit-learn pipelines

---

## ğŸ§  Core Idea
Every supervised learning model minimizes a loss function on training data.  
However, minimizing training loss alone does not guarantee good performance on unseen data.

This project explores how:
- **Simple models** suffer from high bias (underfitting)
- **Complex models** suffer from high variance (overfitting)
- **Regularization** constrains model complexity
- **Cross-validation** estimates true generalization error

---

## ğŸ—‚ï¸ Project Structure

bias-variance-demo/
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ generate_data.py
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ underfit.py
â”‚ â”œâ”€â”€ overfit.py
â”‚ â”œâ”€â”€ regularization.py
â”‚ â””â”€â”€ cross_validation.py
â”‚
â”œâ”€â”€ utils/
â”‚ â””â”€â”€ plotting.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## ğŸ“Š Dataset
A synthetic non-linear dataset is generated using a sine function with Gaussian noise:

y = sin(x) + Îµ

Why synthetic data?
- Eliminates ambiguity from real-world noise
- Makes biasâ€“variance behavior visually clear
- Enables controlled experimentation

---

## ğŸ”´ Underfitting (High Bias)
**Model:** Polynomial Regression (degree = 1)

**Behavior:**
- Model is too simple to capture non-linear structure
- High training error
- High validation error

**Insight:**  
Strong assumptions lead to systematic errors.

---

## ğŸ”µ Overfitting (High Variance)
**Model:** Polynomial Regression (degree = 15)

**Behavior:**
- Model fits noise in the training data
- Very low training error
- Poor generalization to unseen data

**Insight:**  
Memorization replaces learning when model complexity is too high.

---

## ğŸ§© Regularization (Controlling Model Complexity)
To reduce overfitting, a penalty term is added to the loss function:

Loss = Data Loss + Î» Ã— Regularization Term

### Ridge Regression (L2)
- Penalizes squared magnitude of weights
- Produces smooth, stable models
- Retains all features

### Lasso Regression (L1)
- Penalizes absolute magnitude of weights
- Forces some weights to zero
- Performs implicit feature selection

**Engineering Trade-off:**
- Ridge â†’ stability
- Lasso â†’ sparsity

---

## ğŸ” Cross-Validation (Model Selection)
K-Fold Cross-Validation is used to estimate generalization error by:
- Training on Kâˆ’1 folds
- Validating on the remaining fold
- Repeating this process K times

The resulting error curve clearly shows:
- High error at low complexity (underfitting)
- Minimum error at optimal complexity
- Rapid error increase at high complexity (overfitting)

This is how model complexity is selected in real-world ML systems.

---

## ğŸ› ï¸ Tools & Technologies
- Python
- NumPy
- Matplotlib
- scikit-learn
- Pipeline-based modeling
- K-Fold Cross-Validation

---

## ğŸ“š Key Takeaways
- Training accuracy alone is misleading
- Generalization error matters more than training loss
- Model complexity must be controlled
- Regularization reduces variance in a principled way
- Cross-validation is essential for reliable evaluation

---

## ğŸ”® Possible Extensions
- Learning curves (training vs validation loss)
- Logistic regression and classification version
- Hyperparameter tuning using GridSearchCV
- Biasâ€“variance analysis in neural networks

---

## ğŸ‘¤ Author : ItsTheMaverick 
Built to demonstrate **engineering-level understanding of Machine Learning fundamentals**, with emphasis on clarity, rigor, and real-world modeling practices.

â­ If you found this project useful, consider starring the repository.


